{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 6:Creating Hands Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaTWIwzsjnwm"
      },
      "source": [
        "Import necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXOkuxFh2ovI"
      },
      "source": [
        "#Use Javascript code to call your local webcam\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "#Convert the image type that captured by webcam\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import io\n",
        "# OpenCV library \n",
        "import cv2 \n",
        "# plot the img\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNqMk1bMecSC"
      },
      "source": [
        "### Mount your google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeH3mumsENhD",
        "outputId": "375837d9-cb01-4033-852b-1f9678da56b0"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1wvPHprENAf"
      },
      "source": [
        "# Load cascades using v2.CascadeClassifier\n",
        "faces_cascades = cv2.CascadeClassifier(\"/content/drive/MyDrive/Computer_Vision/haarcascade_frontalface_default.xml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhXHldfLeI-C"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "### The following code takes care of using your webcame, take a capture and send it to collab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzUa1IW8xjSq"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "\n",
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.20);\n",
        "\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEZcOgx5jzoT"
      },
      "source": [
        "### Define the Image Conversion function\n",
        "\n",
        "For VideoCapture functions return images with base64 type\n",
        "\n",
        "For cv2 need images with bytes type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vVQFfsTMlsq"
      },
      "source": [
        "def b64_to_bytes(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def bytes_to_b64(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIT9TIIl9DHO"
      },
      "source": [
        "# This function scales a rotated rectangle by a factor of scale_x (width) and scale_y (height)\n",
        "def scale_contour(pts, scale_x, scale_y):\n",
        "    M = cv2.moments(pts)\n",
        "\n",
        "    if M['m00'] == 0:\n",
        "      return pts\n",
        "\n",
        "    cx = int(M['m10']/M['m00'])\n",
        "    cy = int(M['m01']/M['m00'])\n",
        "\n",
        "    cnt_norm = pts - [cx, cy]\n",
        "    cnt_scaled = cnt_norm * np.array([scale_x, scale_y])\n",
        "    cnt_scaled = cnt_scaled + [cx, cy]\n",
        "    cnt_scaled = cnt_scaled.astype(np.int32)\n",
        "\n",
        "    return cnt_scaled\n",
        "\n",
        "# Will be needed in task 6 maybe\n",
        "def crop_hand(pts, im_width, im_height):\n",
        "  x_tl, y_tl = max(0, min(pts[:, 0])), max(0, min(pts[:, 1]))\n",
        "  x_br, y_br = min(im_width, max(pts[:, 0])), min(im_height, max(pts[:, 1]))\n",
        "\n",
        "  return(x_tl, y_tl),(x_br, y_br)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y6uOPbYEYAV"
      },
      "source": [
        "# Detect faces using cascades\n",
        "def detect_faces(img, cascades):\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  faces = cascades.detectMultiScale(gray, 1.3, 4)\n",
        "  return faces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71q1AaEwEFDU"
      },
      "source": [
        "# Capture a frame used for histogramm backprojection\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "while True:\n",
        "  response = input(\"Type anything when ready!:\")\n",
        "  byte = eval_js('capture()')\n",
        "  im = b64_to_bytes(byte)\n",
        "  im_copy = im.copy()\n",
        "  faces = detect_faces(im, faces_cascades)\n",
        "\n",
        "  if len(faces) == 1:\n",
        "    face = faces[0]\n",
        "    cv2.rectangle(im_copy,(face[0],face[1]),(face[0] + face[2], face[1] + face[3]),(0,255,0),2)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im_copy)))\n",
        "  response = input(\"Use this bounding box? [y or n]:\")\n",
        "  if response == 'y':\n",
        "    frame = im[face[1]:face[1]+face[3], face[0]:face[0]+face[2]]\n",
        "    eval_js('showimg(\"{}\")'.format(bytes_to_b64(frame)))\n",
        "    tracking_window_face = face # Initial tracking window for the face\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7bzhsDbopqk"
      },
      "source": [
        "def show_hist(hist):\n",
        "  bin_count = hist.shape[0]\n",
        "  bin_w = 24\n",
        "  img = np.zeros((256, bin_count*bin_w, 3), np.uint8)\n",
        "  for i in range(bin_count):\n",
        "      h = int(hist[i])\n",
        "      cv2.rectangle(img, (i*bin_w+2, 255), ((i+1)*bin_w-2, 255-h), (int(180.0*i/bin_count), 255, 255), -1)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n",
        "  cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEOgsy8IhEZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "7b4e730e-dc78-4743-9bba-ba76726d4a7d"
      },
      "source": [
        "# Compute the the histogram\n",
        "# Transform the frame into HSV\n",
        "frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "# Create an inRange mask for pixels. Limit the saturation in [64., 255.] and brightness in [32., 200.]\n",
        "# only contains white and black color\n",
        "mask = cv2.inRange(frame_hsv, np.array((0., 64., 32.)), np.array((180., 255., 200.)))\n",
        "# Compute the histogram of the frame (use only the HUE channel). See `https://bit.ly/3pdVUEd`\n",
        "# Take into account only pixels which are not too bright and not too dark (use the previous mask)\n",
        "# Use 16 bins and speicfy the range of the hue ([0, 180])\n",
        "frame_hist = cv2.calcHist([frame_hsv], [0], mask, [16], [0,180])\n",
        "# Normalize the histogram between 0 (lowest intensity) and 255 (highest intensity) (use MinMax normalization `cv.NORM_MINMAX`) using the method `https://bit.ly/3jMGhCj`\n",
        "frame_hist = cv2.normalize(frame_hist, frame_hist, 0, 255, cv2.NORM_MINMAX)\n",
        "# Reashape the histogram into a 1-D array (use `.reshape(-1)`)\n",
        "frame_hist = frame_hist.reshape(-1)\n",
        "\n",
        "# Show the histogram\n",
        "show_hist(frame_hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAIAAAA1FTYwAAAD00lEQVR4nO3WMUokUQBF0S7pTYiRuAwxklmGGInLEJchRuIyxGhwGTKRzC4sA/1oZvCbuTCek3QFXY9fDXXpZQP/n9N16vbHZUfn4Bt79QGAn0uAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJDZ1geAL+7WqdvPlx2dg3/EPyAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZLbvH+vcyjIu1rO5nfux8zC38+vj4mXuwQ7Gg13N7VyPncO5nT+fP/Tx1NDyNHZu5nYuPy5e5x5sb/n+O4X9uTfj73gzruZ2rsfO89zO0dhZN79ndpbNydi5mNu5fQO+piEEUhbbDQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=384x256 at 0x7F3751064710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUCWZbclo0yL",
        "outputId": "7bc67192-9f10-4d0d-c0aa-e6d34856af0e"
      },
      "source": [
        "tracking_window_face"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([204, 179, 196, 196], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMwZU2iuvBGT"
      },
      "source": [
        "sv = tracking_window_face "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Co3bmr7Jto"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/Computer_Vision/DataSet/C_32/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBZOoYJnoEto"
      },
      "source": [
        "tracking_window_face = sv  \n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "\n",
        "byte = eval_js('capture()')\n",
        "im = b64_to_bytes(byte)\n",
        "im_width, im_height = im.shape[1],im.shape[0]\n",
        "tracking_window_hand = (0,0,im_width,im_height) # Define the initial tracking window for the hand. It spans the entire caption\n",
        "cpt = -1\n",
        "\n",
        "while True:\n",
        "  byte = eval_js('capture()')\n",
        "  im = b64_to_bytes(byte)\n",
        "  # Transform the image into HSV\n",
        "  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)  \n",
        "  # Compute the standard not too bright not too dark mask\n",
        "  mask = cv2.inRange(hsv, np.array((0., 64., 32.)), np.array((180., 200., 200.)))\n",
        "  # Back project the histogramm on the hsv img\n",
        "  prob = cv2.calcBackProject([hsv], [0], frame_hist, [0,180], scale=1) \n",
        "  # Apply the mask\n",
        "  prob = prob & mask\n",
        " \n",
        "  # First look up for the face using cam shift starting from `tracking_window_face`\n",
        "  (x,y,w,h) = tracking_window_face\n",
        "  bbox, tracking_window_face = cv2.CamShift(prob, tracking_window_face, term_crit)\n",
        "\n",
        "  # Retrieve the rotated bounding rectangle\n",
        "  pts = cv2.boxPoints(bbox).astype(np.int)\n",
        "  # Scale the rotated bounding box 1.5x times  \n",
        "  scaled_pts = scale_contour(pts, 1.5, 1.5)# Use `scale_contour`\n",
        "  # Fill the rotated face bounding box with 0 in the prob map\n",
        "  # Use `cv2.fillPoly`\n",
        "  cv2.fillPoly(prob, [scaled_pts], 0)\n",
        "  # Draw the boundix box around the face\n",
        "  #cv2.polylines(prob, [pts], True, (255, 255, 255), 2)\n",
        "  # Draw the scaled boundix box around the face\n",
        "  cv2.polylines(im, [scaled_pts], True, (255, 0 , 0), 2)\n",
        "\n",
        "  # Now look up for the hand using cam shift starting from `tracking_window_hand`\n",
        "  bbox, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n",
        "  pts = cv2.boxPoints(bbox).astype(np.int)\n",
        "  # Scale the contour around the hand\n",
        "  pts = scale_contour(pts, 1.5, 1.5)\n",
        "  \n",
        "  #Detect hand\n",
        "  cropped_hand_bbox = crop_hand(pts, im_width, im_height)\n",
        " \n",
        "  cv2.rectangle(im, cropped_hand_bbox[0], cropped_hand_bbox[1], (0, 255, 0), 2)\n",
        "  \n",
        "  if cropped_hand_bbox[1][1] !=0:\n",
        "\n",
        "    cropped_hand = cv2.cvtColor(im[cropped_hand_bbox[0][1]:cropped_hand_bbox[1][1], cropped_hand_bbox[0][0]:cropped_hand_bbox[1][0]], cv2.COLOR_BGR2GRAY)\n",
        "    cropped_hand = cv2.resize(cropped_hand, (32, 32))\n",
        "    cpt += 1\n",
        "    #print(cpt)\n",
        "    cv2.imwrite(PATH + str(cpt)+'.jpg', cropped_hand)\n",
        "  \n",
        "  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ6kUVQr6Ynd"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itvcA6MrmS6i"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Computer_Vision/DataSet/C_32/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hcfRV32mWk4",
        "outputId": "f2b5432e-cc64-4ca1-b1f1-2f435d65ae98"
      },
      "source": [
        "print(len(os.listdir(\".\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AlZMk4kmcHw"
      },
      "source": [
        "for count, filename in enumerate(os.listdir(\".\")):\n",
        "  if filename[-3:] == \"jpg\":\n",
        "    plimg = Image.open(filename)\n",
        "    out = plimg.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    out.save(filename[:-4] + '_flipped' + '.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nno1t8InA6V",
        "outputId": "dcc8d52b-17ec-415a-900b-66cd49d7165d"
      },
      "source": [
        "print(len(os.listdir(\".\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1WGstffnC6U"
      },
      "source": [
        "file =\"102_flipped.jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZRCHP5BAAVtt",
        "outputId": "1156595f-cda8-4b22-e5ee-7a1ec4265498"
      },
      "source": [
        "file[-5:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'d.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQpC3OgeAYfC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}